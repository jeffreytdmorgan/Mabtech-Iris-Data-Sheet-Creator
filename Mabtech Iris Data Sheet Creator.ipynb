{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello and welcome to the Mabtech Iris Data Sheet Creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRO: \n",
    "This script works specifically for the Mabtech Iris machine's folder output. At the end of this script you should have a Master Excel file that summarizes the results within all of your selected IRIS folders. The script currently takes in triplicates and produces an average, standard deviation, SFC per million, SI, Poisson per well in triplicate, p-value (based on T test), and determines significance based on these. If any errors come up that the script does not address, please let me know: jmorgan@lji.org. \n",
    "\n",
    "BEFORE YOU START:\n",
    "The script is based on particular assumptions about your Fluorospot Plate layout. Please refer to the included Word document titled \"Python-Mabtech Iris Data Sheet Creator Plate Layout\" if you are unsure whether your plate will work. As you scroll through the script, you will notice light blue lines led by #s within each box. These are instructions or explanations of what each line of code is doing. If you are interested in changing the script, you can refer to the #s to gleam the purpose of each line. Otherwise, the only absolutely necessary box to pay attention to is box #2 discussed below.\n",
    "\n",
    "INPUT NEEDED BY YOU:\n",
    "In order for this script to work correctly, please read and follow the instructions following the #s in box #2. If the sentence is cut off by the box, scroll to the left to continue reading the instructions.\n",
    "\n",
    "Once you have filled in the appropriate information in box #2, scroll up and look for a drop down menu labelled \"Cell\" and click it, then scroll down to \"Run All\" and click it. You should now see an * in place of the number within the brackets to the left of each box. This means the script is running. When the * is replaced by a number, that means that particular box has finished running. If the script has an error pop up after the * turns to a number, refer to the next paragraph.\n",
    "\n",
    "ERROR HANDLING:\n",
    "There are multiple types of errors that can occur. When an error occurs, the box that the error occurs in will have a number within the brackets to the left of the box and the following box will have an empty pair of brackets. \n",
    "\n",
    "Most likely this error has to do with wording, so first look back at your inputs in box #2 and double check that the spelling and capitilization correctly reflects your folders names. If nothing seems to be out of order here, refer to the #s within the problematic box and see if there are any instructions on how to fix the error. Lastly, you can try and google the error that pops up after the box fails to run (the last line in red is all that is needed to look it up). \n",
    "\n",
    "One thing to note: if you change something within a box AFTER you ran the box prior, you will have to RErun all the boxes after the changed box to see a changed result.\n",
    "\n",
    "If an error occurs that you cannot solve using the error handling methods listed above, please email me the issue and I will do my best to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#Always run this box first before trying to run any other boxes. \n",
    "#Only needs to be run once per session.\n",
    "import os\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the number of donors ran on single fluorospot plate: 1\n",
      "Please enter the exact well locations of the negative wells for one donor (note: capitalize letter and seperate individual wells with a comma and no space): H10,H11,H12\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#This box will contain pop-ups that need to be answered\n",
    "#The first pop-up will ask the number of donors used PER PLATE. \n",
    "#Please click in the box pop-up below and type the number used (use numbers, dont spell it out) then click enter\n",
    "#From the number you responded with, multiple pop-ups will come up (one at a time)\n",
    "#Please input the exact well locations of the negatives for the FIRST DONOR, then the next well locations for the following donor (etc)\n",
    "#To type in the well locations, include the row letter capitalized and the column number, seperate the individual well locations with a comma without a space\n",
    "#Example: 4 donors with all the negatives in the last row \n",
    "#Type \"4\" (without quotes) in the first pop-up then click enter.\n",
    "#Another pop-up will come up, type in the well locations for the first donor, \"H1,H2,H3\" (without quotes), then press enter\n",
    "#Another pop-up will come up, type int he well locations for the second donor, \"H4,H5,H6\" (without quotes), then press enter\n",
    "#Repeat until all the donor's negative locations appear below.\n",
    "#If a mistake was made and you already entered the mistake, re-run this box\n",
    "\n",
    "#This is an object that takes in a user's input and makes it into an integer-type object\n",
    "number_of_donors = int(input(\"Please enter the number of donors ran on single fluorospot plate: \"))\n",
    "\n",
    "#Lists of the responses that will be manipulated later\n",
    "list_of_negs=[]\n",
    "Master_neg_list= []\n",
    "donor_name_list=[]\n",
    "\n",
    "#This i will be used as a starting location for the while loop\n",
    "i=0\n",
    "\n",
    "#This if statement is just to catch a mistake number of donors (0 or negative numbers)\n",
    "if number_of_donors > 0:\n",
    "    \n",
    "    #The while loop is used to enter multiple negative locations based on the number of donors inputed\n",
    "    #Essentially i divided by the number inputed will only be equal to 1 when they're the same number\n",
    "    #So this loop starts at 0 then counts up everytime the variable is not equal to one\n",
    "    while i/number_of_donors != 1: \n",
    "        \n",
    "        #Another input variable that will be the filled in with each negative well location per donor\n",
    "        #This will pop up each time the well loop runs, so for the # of donors used per plate\n",
    "        negative_control=list(input(\"Please enter the exact well locations of the negative wells for one donor (note: capitalize letter and seperate individual wells with a comma and no space): \").split(\",\"))\n",
    "        \n",
    "        #Save to list\n",
    "        list_of_negs.append(negative_control)\n",
    "        \n",
    "        #Only saving last well location to this list\n",
    "        Master_neg_list.append(negative_control[-1])\n",
    "        \n",
    "        #Creating a special donor name per negative list\n",
    "        donor_name=\"donor\"+str((i+1))\n",
    "        \n",
    "        #Saving donor name\n",
    "        donor_name_list.append(donor_name)\n",
    "        \n",
    "        #Once the while loop is ran, this increases i by one so it will count up\n",
    "        i=i+1\n",
    "else:\n",
    "    print(\"DONOR INPUT INVALID: must be 1 or more\")\n",
    "    \n",
    "flat_neg_list = [item for sublist in list_of_negs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "#This is the most complicated box, but it is necessary.\n",
    "#Please fill in the below variables, all that is needed for you to change will be on the RIGHT side of equals sign.\n",
    "#Once filled in, run this box.\n",
    "\n",
    "#a\n",
    "#Create the path to wherever your Iris folders or Iris excel sheets are. \n",
    "#Pay close attention to capitalization and spaces.\n",
    "#Use / to indicate the next step in the path.\n",
    "#The starting location of the path is the next step after the location of this program.\n",
    "#So if you dragged this program, \"Mabtech Iris Data Sheet Creator,\" to your Desktop, \"Desktop\" WOULD NOT be used below.\n",
    "#If you dragged this program into a folder, say a master folder that contains other folders of data, that folder name WOULD NOT be included below.\n",
    "#The example below goes into a folder labelled \"Python for Public\", then ends in the folder \"DATA\" which contains all the iris output folders.\n",
    "\n",
    "folder_path = \"TB Deconvolution One Negative\"\n",
    "\n",
    "#b\n",
    "#Fill in the number of cells you plated per well using numbers (don't spell out number).\n",
    "#DO NOT use quotes.\n",
    "#In the example below, PBMCs are plated at 50uL per well starting from 4 million cells per mL concentration.\n",
    "\n",
    "Number_of_cells_per_well = 200000\n",
    "\n",
    "#c\n",
    "#Fill in the minimum number of sfc hits acceptable to be considered a positive hit.\n",
    "#In the example below, positive triplicates are only considered significant in the last column if their net sfcs are above 20, otherwise it is considered negative.\n",
    "\n",
    "Number_of_sfcs_necessary_to_be_considered=20\n",
    "\n",
    "#d\n",
    "#This is the blank master excel sheet that will be filled out through the script.\n",
    "#Fill in the location (same way as folder_path above was filled in) and name what you want the end excel file to be.\n",
    "#Only fill in what is between the first quotation marks, do not change the engine.\n",
    "#If this program is located on your desktop and you want the resulting excel file to be located there, just create a name for the excel file within the quotes.\n",
    "#Make sure to END WITH .xlsx to your title name, otherwise it will not appear.\n",
    "\n",
    "writer = pd.ExcelWriter('TB_Decon_ME_1Neg.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 functional excel files found and added to dataframe\n",
      "0 disfunctional excel files found in named file list (first for loop in this box): []\n",
      "0 disfunctional excel files found in files in named file list (second for loop in this box): []\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "#This creates a directory of the iris folders using the path made above.\n",
    "Mabtech_Iris_Folders_dir = os.listdir(folder_path) \n",
    "\n",
    "#This empty list will be filled with your data.\n",
    "df_list = []\n",
    "error_issue_list1=[]\n",
    "error_issue_list2=[]\n",
    "\n",
    "#This begins a loop searchig through the folder you labelled in the path.\n",
    "for option in Mabtech_Iris_Folders_dir:\n",
    "   \n",
    "    #This is looking at the folders/files in the path and checking for excel files, if it finds them it will continue to the next step.\n",
    "    if option.endswith(\".xlsx\"):\n",
    "        \n",
    "        try:\n",
    "            #If excel files are found, it will create a DataFrame with the data found on sheet 2 of the mabtech excel files.\n",
    "            path_new = folder_path + \"/\" + option\n",
    "            Excel_Data=pd.read_excel(path_new,sheet_name=1, usecols=\"A:H\")\n",
    "\n",
    "            #This adds the Dataframe to the empty list.\n",
    "            df_list.append(Excel_Data)\n",
    "        \n",
    "        #This will add any files with issues (corrupted) into a folder that will be mentioned later\n",
    "        except:\n",
    "            error_issue_list1.append(option)\n",
    "   \n",
    "    #If no excel files are found, this will go into the folders within the path and look for excel files.\n",
    "    else:\n",
    "        \n",
    "        #This avoids hidden files to prevent them from breaking the code.\n",
    "        if not option.startswith(\".\"):\n",
    "            excel_files_in_folder = []\n",
    "            for file in os.listdir(folder_path+\"/\"+option):\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    try:\n",
    "                        #After going into the next folder looking for the excel file, it will create a Dataframe from the found excel.\n",
    "                        excel_files_in_folder.append(file)\n",
    "                        path_new = folder_path + \"/\" + option + \"/\" + file\n",
    "                        Excel_Data=pd.read_excel(path_new,sheet_name=1, usecols=\"A:H\")\n",
    "\n",
    "                        #This adds the Dataframe to the empty list\n",
    "                        df_list.append(Excel_Data)\n",
    "                    except:\n",
    "                        error_issue_list2.append(file)\n",
    "                    \n",
    "            #Because Mabtech Iris Folders only contain one excel per output, I incorporated this check to ensure only one excel file appears within the folder.\n",
    "            if len(excel_files_in_folder) != 1:\n",
    "                print(\"Check the following folder(s) for duplicate excel files: \" + option)\n",
    "                \n",
    "#These print out if issues were found and the number of functional files found\n",
    "print(str(len(df_list))+\" functional excel files found and added to dataframe\")\n",
    "print(str(len(error_issue_list1))+\" disfunctional excel files found in named file list (first for loop in this box): \"+ str(error_issue_list1))\n",
    "print(str(len(error_issue_list2))+\" disfunctional excel files found in files in named file list (second for loop in this box): \"+str(error_issue_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "#This box defines a function that creates a list of headers within a dataframe list.\n",
    "#It will be used as a simple check to make sure the correct headers were read in to a dataframe.\n",
    "def check_headers(df_list):\n",
    "    Col_Names=[]\n",
    "    \n",
    "    #Start of a for loop that will look through the newly created list.\n",
    "    for dataframes in df_list:\n",
    "        \n",
    "        #This creates an object, x, which contains the list of column headers.\n",
    "        x=list(dataframes.columns)\n",
    "        Col_Names.append(x)\n",
    "        \n",
    "    #This ends the function and tells it what to output when it is ran (in this case the list of headers).    \n",
    "    return(Col_Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#This function is needed to run unequal columns together.\n",
    "#If you have multiple Negative triplicates, this will ensure the longer row wont cause a break in the script.\n",
    "def blank_filler(new_df_row):\n",
    "    \n",
    "    #This next section creates a loop that fills in blanks (NaN) until the new df row is equal to the New_col list.\n",
    "    while ( len(new_df_row) < len( New_col) ):\n",
    "        \n",
    "        #The word None is used here to fill in NaN variables that python will recognize as null data rather than a 0.\n",
    "        #This is imprtant bc if anything else is used (string-wise) the column will no longer be seen as a data column by python.\n",
    "        new_df_row.append(None)\n",
    "        \n",
    "    return (new_df_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "#This creates a basic Dataframe that includes triplicates based on the well.\n",
    "\n",
    "#This creates an empty dictionary which will be used to associate triplicates to wells.\n",
    "letter_dict = {}\n",
    "letter_str='ABCDEFGH'\n",
    "\n",
    "#This for loop associates individual letters to a number that will be used to call on its location.\n",
    "for i in range(0,len(letter_str)):\n",
    "    letter = letter_str[i]\n",
    "    letter_dict[i]=letter\n",
    "\n",
    "well_list=[]\n",
    "trip_marker_list=[]\n",
    "\n",
    "#This for loop starts to number the 96 well plate.\n",
    "for well_number in range(0,96):    \n",
    "    col_number = (well_number % 12) + 1\n",
    "\n",
    "    row_index = int(well_number/12)\n",
    "    row_letter = letter_dict[row_index]\n",
    "\n",
    "    #Calculate triplicates as if numbered across the plate.\n",
    "    trip_across = int(well_number/3) + 1\n",
    "    row_offset = -(row_index*3)\n",
    "    col_offset = int((col_number-1)/3)*7\n",
    "\n",
    "    #Add row & column offsets to the triplicate value (across) to find triplicate value when going down the plate.\n",
    "    trip_down = (trip_across + row_offset + col_offset)\n",
    "    \n",
    "    #The well now matches the Well output from Iris.\n",
    "    well=row_letter+ str(col_number)\n",
    "\n",
    "    well_list.append(well)\n",
    "   \n",
    "    trip_marker_list.append(trip_down)\n",
    "\n",
    "    \n",
    "#This is the DF that will be used to create a triplicate column in the future Dfs\n",
    "trip_df_marker=pd.DataFrame()\n",
    "trip_df_marker[\"Well\"]=well_list\n",
    "trip_df_marker[\"Triplicate_#\"]=trip_marker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "#This creates a dataframe for the negatives per plate\n",
    "#empty dataframe\n",
    "neg_finder_df=pd.DataFrame()\n",
    "\n",
    "#adding two columns created earlier, donor list and well list\n",
    "neg_finder_df[\"Donor_number\"]=donor_name_list\n",
    "neg_finder_df[\"Well\"]=Master_neg_list\n",
    "\n",
    "#creating a triplicate number for these donors, starting with an empty list\n",
    "triplicate_fordonor_list=[]\n",
    "\n",
    "#for loop based on specific well locations\n",
    "for donorwell in neg_finder_df[\"Well\"]:\n",
    "    \n",
    "    #only showing the unique well locations so no repeats\n",
    "    special_list=trip_df_marker[\"Well\"].unique()\n",
    "    \n",
    "    #if the specific well location is in the unique list then...\n",
    "    if donorwell in  special_list:\n",
    "        \n",
    "        # then create a small DF on that donorwell\n",
    "        special_df=trip_df_marker.loc[(trip_df_marker[\"Well\"]==donorwell)]\n",
    "        triplicate_fordonor_list.append(special_df)\n",
    "        \n",
    "#attach all the individual DF together into one and on each well        \n",
    "hopefully_neg_df=pd.concat(triplicate_fordonor_list)\n",
    "specialized_neg_df=pd.merge(neg_finder_df, hopefully_neg_df, on=\"Well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "#applies the negative DF to the Triplicate DF\n",
    "#these will be the names for the columns of the new DF\n",
    "New_headers_for_trip=[\"Donor_number\", \"Triplicate_#\"]\n",
    "neg_df_trip_list=list(specialized_neg_df[\"Triplicate_#\"])\n",
    "neg_df_donornum_list=list(specialized_neg_df[\"Donor_number\"])\n",
    "\n",
    "Triplicate_forDonor=trip_df_marker[\"Triplicate_#\"].unique()\n",
    "\n",
    "Donor_number_by_triplicate=[]\n",
    "\n",
    "#for loop based on the triplicate DF \n",
    "for Tnumber in Triplicate_forDonor:\n",
    "    \n",
    "    #if that number is greater than the biggest triplicate number of the last donor in the negative DF, then...\n",
    "    if Tnumber>neg_df_trip_list[-1]:\n",
    "        \n",
    "        #...no donor is associated with it\n",
    "        Donor_number_by_triplicate.append(None)\n",
    "    \n",
    "    #the else statement will now be dealing with the triplicates that are associated with donors\n",
    "    else:\n",
    "        \n",
    "        #another counting while loop\n",
    "        i = 0 \n",
    "        while i < (len(specialized_neg_df[\"Triplicate_#\"])) :\n",
    "            \n",
    "            #This is why the well order is important for input.\n",
    "            #This will start with the first negative triplicate put in, which needs to be the first one encountered on the plate\n",
    "            if Tnumber <= neg_df_trip_list[i]:\n",
    "                    Donor_number_by_triplicate.append(neg_df_donornum_list[i])\n",
    "                    \n",
    "                    #this break prevents an endless loop from happening\n",
    "                    break\n",
    "            \n",
    "            #The count will only occur once every triplicate that is less than the first donor triplicate is reached\n",
    "            else:\n",
    "                i=i+ 1\n",
    "                \n",
    "#Finishing up incorporating donor number into the triplicate DF\n",
    "Donor_number_and_triplicate_df = pd.DataFrame(list(zip(Donor_number_by_triplicate, Triplicate_forDonor)), columns =New_headers_for_trip)\n",
    "Trip_donornum_df=pd.merge(Donor_number_and_triplicate_df,trip_df_marker,how=\"inner\",on=(\"Triplicate_#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "#This box changes the triplicate column from int to str so I can call on negative wells by name\n",
    "Neg_trip_list=[]\n",
    "\n",
    "#isolating only the unique well numbers, so no repeats\n",
    "Trip_forneg_list=Trip_donornum_df[\"Well\"].unique()\n",
    "\n",
    "#for loop using the unique wells\n",
    "for neg_well in Trip_forneg_list:\n",
    "    i=0\n",
    "    \n",
    "    #this while loop is going to use all the negatives regardless of donor number to make the triplicates into \"neg\"\n",
    "    while i < (len(flat_neg_list)):\n",
    "        if neg_well ==flat_neg_list[i]:\n",
    "            each_well_df=Trip_donornum_df.loc[(Trip_donornum_df[\"Well\"]==neg_well)]\n",
    "            Neg_trip_list.append(int(each_well_df[\"Triplicate_#\"].unique()))\n",
    "            break\n",
    "        else:\n",
    "            i=i+1\n",
    "\n",
    "Trip_donornum_completed_df=Trip_donornum_df.replace(Neg_trip_list,\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "#This list is what each column header should be when iris creates the file.\n",
    "Expected_Cols=[\"Plate\", \"Well\",\"Read Date\", \"Saved Date\", \"Machine ID\", \"Analyte Secreting Population\", \"LED Filter\", \"Spot Forming Units (SFU)\"]\n",
    "\n",
    "#This object is the datafile list after the function check_headers has been run.\n",
    "Col_headers=check_headers(df_list)\n",
    "\n",
    "#This for loop checks if each header within the excel files are what should be expected from the iris output.\n",
    "for col_head in Col_headers:\n",
    "    \n",
    "    #If the headers are other than what is expected, there is an excel file that is not an iris output or is a broken iris excel file.\n",
    "    assert col_head == Expected_Cols, \"Column headers within one or more excel files are not as expected: \"+col_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LED490 Total', 'LED490 Single', 'LED550 Total', 'LED550 Single', 'LED640 Total', 'LED640 Single', 'LED490+LED640', 'LED550+LED640', 'LED490+LED550', 'LED490+LED550+LED640']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'DonorTP0030_Pool0238_Run01': 960,\n",
       "         'DonorTP0031_Pool0054_Run01': 960,\n",
       "         'DonorTP0012_Pool0570_Run01': 960,\n",
       "         'DonorTP0031_Pool0780_Run01': 960,\n",
       "         'DonorTP0012_Pool1047_Run01': 960,\n",
       "         'DonorTP0031_Pool0243_Run01': 960,\n",
       "         'DonorTP0012_Pool0190_Run01': 960,\n",
       "         'DonorTP0031_Pool0154_Run01': 960,\n",
       "         'DonorTP0012_Pool0554_Run01': 960,\n",
       "         'DonorTP0012_Pool1045_Run01': 960,\n",
       "         'DonorTP0012_Pool0961_Run01': 960,\n",
       "         'DonorTP0012_Pool0218_Run01': 960,\n",
       "         'DonorTP0031_Pool0723_Run01': 960,\n",
       "         'DonorTP0012_Pool0805_Run01': 960,\n",
       "         'DonorTP0012_Pool1046_Run01': 960,\n",
       "         'DonorTP0030_Pool1066_Run01': 960,\n",
       "         'DonorTP0031_Pool0215_Run01': 960,\n",
       "         'DonorTP0031_Pool0029_Run01': 960,\n",
       "         'DonorTP0031_Pool0168_Run01': 960,\n",
       "         'DonorTP0031_Pool0864_Run01': 960,\n",
       "         'DonorTP0012_Pool0039_Run01': 960})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#12\n",
    "#Combines all dataframes made above into one large dataframe and removes the three columns below.\n",
    "complete_df=(pd.concat(df_list, axis=0)).drop(columns=[\"Machine ID\",\"Saved Date\",\"Analyte Secreting Population\"])\n",
    "  \n",
    "LEDcombinations=[]\n",
    "\n",
    "#This isolates the different LED filters that the Mabtech puts out.\n",
    "#This will be used to ensure that no matter how many Fluorospot colors you used (double, triple, etc.), the following script will gather all the data for them.\n",
    "LED_list=complete_df[\"LED Filter\"].unique()\n",
    "for LED in LED_list:\n",
    "    LEDcombinations.append(LED)\n",
    "\n",
    "#This prints out the list of unique found combinations found within the LED Filter column of the Mabtech excel output.\n",
    "#If it is not what you expected, please contact me.\n",
    "print(LEDcombinations)\n",
    "\n",
    "#This prints out the number of times a specific plate name shows up.\n",
    "#This is helpful in determining where something may have went wrong\n",
    "collections.Counter(complete_df[\"Plate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "#Each of these empty lists will be filled based on the specific LED Filter only.\n",
    "#Though only the Total df list and Single combo df list will be used in the rest of the script, I parcel out the singles and combo into their own list in case theyre needed.\n",
    "Total_dfs_list=[]\n",
    "Singles_list=[]\n",
    "Combo_dfs_list=[]\n",
    "Single_combo_dfs_list=[]\n",
    "LEDCOMBO_headers=[\"Plate\",\"Read Date\",\"Donor_number\",\"Well\",\"Triplicate_#\"]\n",
    "#This defines an object headers, which will be used later.\n",
    "headers=[\"Plate\", \"Well\",\"Read Date\",\"LED Filter\"]\n",
    "\n",
    "#This for loop is using the unique list defined in the previous box to ensure all the different combinations are incorporated.\n",
    "for led in LEDcombinations:\n",
    "    \n",
    "    #The totals will be caught in this loop then added to their appropriate list.\n",
    "    if led.endswith(\"Total\"):\n",
    "        total_df = (complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        Total_dfs_list.append(total_df)\n",
    "    \n",
    "    #The combinations will be caught in this loop then added to their appropriate lists.\n",
    "    if \"+\" in led:\n",
    "        \n",
    "        #This both defines the df by its LED filter and renames the SFU column to its LED filter (useful later).\n",
    "        combo_df=(complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        combo_df_for_combined_df = ((complete_df.loc[complete_df[\"LED Filter\"]==led]).rename(columns={\"Spot Forming Units (SFU)\": led}))\n",
    "        Combo_dfs_list.append(combo_df)\n",
    "        Single_combo_dfs_list.append(combo_df_for_combined_df)\n",
    "        LEDCOMBO_headers.append(led)\n",
    "    \n",
    "    #The singles will be caught in this loop then added to their appropriate lists.\n",
    "    if led.endswith(\"Single\"):\n",
    "        \n",
    "        #This both defines the df by its LED filter and renames the SFU column to its LED filter (useful later).\n",
    "        single_df=(complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        singles_for_combined_df=((complete_df.loc[complete_df[\"LED Filter\"]==led]).rename(columns={\"Spot Forming Units (SFU)\": led}))\n",
    "        Singles_list.append(single_df)\n",
    "        Single_combo_dfs_list.append(singles_for_combined_df)\n",
    "        LEDCOMBO_headers.append(led)\n",
    "        \n",
    "LEDCOMBO_headers.append(\"Sum_of_all_activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "#This box containes a bit of magic I dont fully comprehend, but it combines dataframes based on their headers and LED filters.\n",
    "from functools import reduce\n",
    "single_combo_header=[\"Plate\",\"Well\",\"Read Date\"]\n",
    "New_single_combo_dfs_list=[]\n",
    "\n",
    "#You will see this If loop a few times.\n",
    "#If you are running a single-color fluorospot, this loop will skip certain analysis which would break the script.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    for df in Single_combo_dfs_list:\n",
    "        new_df=df.drop(columns=\"LED Filter\")\n",
    "        New_single_combo_dfs_list.append(new_df)\n",
    "    \n",
    "    #The magic line is below. The merge will combine the DFs, but lambda and reduce remove repetitive columns to make the DF cleaner\n",
    "    singles_and_combined_df=reduce(lambda x,y : pd.merge(x, y, on = single_combo_header), New_single_combo_dfs_list)\n",
    "\n",
    "    #This sums up the singles and columns into a new sum row.\n",
    "    sum_column=singles_and_combined_df.sum(axis=1)\n",
    "\n",
    "    singles_and_combined_df[\"Sum_of_all_activation\"]=sum_column\n",
    "\n",
    "    #This runs the triplicate function on the newly created DF.\n",
    "    triplicate_singles_and_combined_df=pd.merge(singles_and_combined_df,Trip_donornum_completed_df,how=\"outer\",on=(\"Well\"))\n",
    "\n",
    "    All_Activation_Summary=triplicate_singles_and_combined_df[LEDCOMBO_headers]\n",
    "\n",
    "    #Now that the Df is made and the new columns are added, this saves it to the master excel sheet.\n",
    "    All_Activation_Summary.to_excel(writer,sheet_name=\"All_Activation_Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "#This step is repeated three times for three different lists of DFs.\n",
    "#A new DF is created based on the triplicates that get assigned by the function created in box #6.\n",
    "trip_Total_dfs_list=[]\n",
    "for df in Total_dfs_list:\n",
    "    new_df=pd.merge(df,Trip_donornum_completed_df,how=\"outer\",on=(\"Well\"))\n",
    "    trip_Total_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "trip_single_dfs_list=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    for df in Singles_list:\n",
    "        new_df=pd.merge(df,Trip_donornum_completed_df,how=\"outer\",on=(\"Well\"))\n",
    "        trip_single_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "trip_combo_dfs_list=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    for df in Combo_dfs_list:\n",
    "        new_df=pd.merge(df,Trip_donornum_completed_df,how=\"outer\",on=(\"Well\"))\n",
    "        trip_combo_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep1', 'rep2', 'rep3']\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "#This is creating headers based on the number of negative wells you defined in box 2.\n",
    "rep_str=\"rep\"\n",
    "Number_of_negative_wells = len(list_of_negs[0])\n",
    "#This makes a list of the word \"rep\" for every negative well you defined.\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "num_rep_list=[]\n",
    "\n",
    "#This for loop will add numbers next to each rep in the list above.\n",
    "for i in range(0, (len(rep_list))):\n",
    "    num_rep_list.append(\"rep\"+str(i+1))\n",
    "print(num_rep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "#This function will be used to isolate specific datum in future triplicates.\n",
    "def get_replicates_from_row(df):\n",
    "    replicates=[]\n",
    "    for replicate_header in num_rep_list:\n",
    "        for sfu in df[replicate_header]:\n",
    "            replicates.append(sfu)\n",
    "        \n",
    "    return(replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plate_ID', 'Read_Date', 'Donor_number', 'Triplicate_#', 'LED_Filter', 'rep1', 'rep2', 'rep3']\n"
     ]
    }
   ],
   "source": [
    "#20 \n",
    "#This step gets repeated three times to three different lists of DFs\n",
    "\n",
    "#This is a new a list of new col headers which will be used for a new df.\n",
    "New_col=[\"Plate_ID\", \"Read_Date\",\"Donor_number\", \"Triplicate_#\",\"LED_Filter\"]\n",
    "\n",
    "#This is a repeat of the rep headers, but it adds it directly to the new headers above\n",
    "rep_str=\"rep\"\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "for i in range(0, (len(rep_list))):\n",
    "    New_col.append(\"rep\"+str(i+1))\n",
    "print(New_col)\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_Total_dfs_list=[]\n",
    "\n",
    "#For loop within the list of total dataframes.\n",
    "for df in trip_Total_dfs_list:\n",
    "    new_total_df_list=[]\n",
    "    \n",
    "    #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "    LED_list=df[\"LED Filter\"].unique()\n",
    "    for ledfilter in LED_list:\n",
    "        ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "        \n",
    "        #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "        plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "            #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "            Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "            for read_date in Read_date_list:\n",
    "                read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "                \n",
    "                donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                for donor in donor_num_list:\n",
    "                    donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "                    \n",
    "                    \n",
    "                    #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                    trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                    for trip in trip_list:\n",
    "                        trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                        #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                        new_rows = [ plate, read_date, donor, trip, ledfilter ]\n",
    "\n",
    "                        #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                        #This for loop will now run through and isolate each SFU by row and name.\n",
    "                        for index, row in trips_df.iterrows():\n",
    "                            \n",
    "                            #With these isolated SFUs, a new object is formed.\n",
    "                            SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "    \n",
    "                            #This object is now added to the new row set made above.\n",
    "                            new_rows.append( SFU )\n",
    "\n",
    "                        #The new row is then added into a list to become a new df.\n",
    "                        new_total_df_list.append(new_rows)\n",
    "\n",
    "        #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "        new_total_df=pd.DataFrame( new_total_df_list , columns = New_col )\n",
    "        \n",
    "        #This adds the new DF to a list of all the new Total Dfs.\n",
    "        new_Total_dfs_list.append(new_total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_single_dfs_list=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    #For loop within the list of total dataframes.\n",
    "    for df in trip_single_dfs_list:\n",
    "        new_single_df_list=[]\n",
    "\n",
    "        #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "        LED_list=df[\"LED Filter\"].unique()\n",
    "        for ledfilter in LED_list:\n",
    "            ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "\n",
    "            #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "            plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "            for plate in plate_list:\n",
    "                plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "                #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "                Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "                for read_date in Read_date_list:\n",
    "                    read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "                    \n",
    "                    donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                    for donor in donor_num_list:\n",
    "                        donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "                    \n",
    "                        #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                        trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                        for trip in trip_list:\n",
    "                            trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                            #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                            new_rows = [ plate, read_date, donor, trip, ledfilter ]\n",
    "\n",
    "                            #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                            #This for loop will now run through and isolate each SFU by row and name.\n",
    "                            for index, row in trips_df.iterrows():\n",
    "\n",
    "                                #With these isolated SFUs, a new object is formed.\n",
    "                                SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "\n",
    "                                #This object is now added to the new row set made above.\n",
    "                                new_rows.append( SFU )\n",
    "\n",
    "                            #The new row is then added into a list to become a new df.\n",
    "                            new_single_df_list.append(new_rows)\n",
    "\n",
    "            #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "            new_single_df=pd.DataFrame( new_single_df_list , columns = New_col )\n",
    "\n",
    "            #This adds the new DF to a list of all the new Total Dfs.\n",
    "            new_single_dfs_list.append(new_single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_combo_dfs_list=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    #For loop within the list of total dataframes.\n",
    "    for df in trip_combo_dfs_list:\n",
    "        new_combo_df_list=[]\n",
    "\n",
    "        #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "        LED_list=df[\"LED Filter\"].unique()\n",
    "        for ledfilter in LED_list:\n",
    "            ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "\n",
    "            #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "            plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "            for plate in plate_list:\n",
    "                plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "                #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "                Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "                for read_date in Read_date_list:\n",
    "                    read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "                    donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                    for donor in donor_num_list:\n",
    "                        donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "                    \n",
    "                        #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                        trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                        for trip in trip_list:\n",
    "                            trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                            #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                            new_rows = [ plate, read_date, donor, trip, ledfilter ]\n",
    "\n",
    "                            #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                            #This for loop will now run through and isolate each SFU by row and name.\n",
    "                            for index, row in trips_df.iterrows():\n",
    "\n",
    "                                #With these isolated SFUs, a new object is formed.\n",
    "                                SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "\n",
    "                                #This object is now added to the new row set made above.\n",
    "                                new_rows.append( SFU )\n",
    "\n",
    "                            #The new row is then added into a list to become a new df.\n",
    "                            new_combo_df_list.append(new_rows)\n",
    "\n",
    "            #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "            new_combo_df=pd.DataFrame( new_combo_df_list , columns = New_col )\n",
    "\n",
    "            #This adds the new DF to a list of all the new Total Dfs.\n",
    "            new_combo_dfs_list.append(new_combo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "#This box is also repeated three times for three different lists of DFs.\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "for df in new_Total_dfs_list:\n",
    "    new_total_df_Average=df.mean(axis=1, skipna=True)\n",
    "    new_total_df_StDev=df.std(axis=1, skipna=True)\n",
    "    \n",
    "    df[\"Average\"]=new_total_df_Average\n",
    "    df[\"StDev\"]=new_total_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    for df in new_single_dfs_list:\n",
    "        new_single_df_Average=df.mean(axis=1, skipna=True)\n",
    "        new_single_df_StDev=df.std(axis=1, skipna=True)\n",
    "\n",
    "        df[\"Average\"]=new_single_df_Average\n",
    "        df[\"StDev\"]=new_single_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else: \n",
    "    combo_header=[\"Plate_ID\",\"Read_Date\",\"Donor_number\",\"Triplicate_#\"]\n",
    "    for df in new_combo_dfs_list:\n",
    "        new_combo_df_Average=df.mean(axis=1, skipna=True)\n",
    "        new_combo_df_StDev=df.std(axis=1, skipna=True)\n",
    "\n",
    "        df[\"Average\"]=new_combo_df_Average\n",
    "        df[\"StDev\"]=new_combo_df_StDev\n",
    "\n",
    "    new_combo_df=reduce(lambda x,y : pd.merge(x, y, on = combo_header), new_combo_dfs_list)\n",
    "    new_combo_df.to_excel(writer,sheet_name=\"Just_double_triple_producers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26\n",
    "#This essentially defines the poisson function to be called later.\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#27\n",
    "#This essentially defines a function as stats to be called later.\n",
    "import math\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28\n",
    "#Using the number of cells you defined per plate earlier, this function creates the net sfu number from an average.\n",
    "def get_netsfu(average):\n",
    "    num_to_get_to_mil=1000000/Number_of_cells_per_well\n",
    "    net_sfu=((average-Neg_Avg)*num_to_get_to_mil)\n",
    "    if net_sfu > 0:\n",
    "        return(net_sfu)\n",
    "    if net_sfu<0:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "#This box runs several equations and creates multiple columns.\n",
    "\n",
    "#A lot of what is happening is similar to box 13, so I only explain the new parts.\n",
    "statistics_Total_dfs_list=[]\n",
    "for df in new_Total_dfs_list:\n",
    "    total_NetSFC=[]\n",
    "    total_SI_list=[]\n",
    "    total_poisson_Rep1_list=[]\n",
    "    total_poisson_Rep2_list=[]\n",
    "    total_poisson_Rep3_list=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "                    \n",
    "                #Using the read date DF, the negative averages will be made.\n",
    "                Neg_df=donornum_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "                \n",
    "                Neg_Avg=float(Neg_df[\"Average\"])\n",
    "\n",
    "                #If the neg average is above or equal to 2, mu for poisson will use it.\n",
    "                if Neg_Avg>=2:\n",
    "                    mu=Neg_Avg\n",
    "\n",
    "                #Otherwise mu will be defined as 2.\n",
    "                else:\n",
    "                    mu=2\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    Average_list=trips_df[\"Average\"]\n",
    "                    for average in Average_list:\n",
    "\n",
    "                        #This runs the netsfu function then uses it to make the SI\n",
    "                        netsfc=get_netsfu(average)\n",
    "                        total_NetSFC.append(netsfc)\n",
    "\n",
    "                        #As long as the Neg average is above 0, the average of each triplicate will be divided by it.\n",
    "                        if Neg_Avg > 0:\n",
    "                            SI=(average/Neg_Avg)\n",
    "                            total_SI_list.append(SI)\n",
    "\n",
    "                        #Otherwise SI is just the average of the triplicate.\n",
    "                        else:\n",
    "                            zero_neg_SI=average\n",
    "                            total_SI_list.append(zero_neg_SI)\n",
    "\n",
    "                    Rep1=trips_df[\"rep1\"]\n",
    "                    #The next for loops are running poisson.\n",
    "                    for rep1 in Rep1:\n",
    "                        if rep1>=1:\n",
    "                            x1=rep1-1\n",
    "                        else:\n",
    "                            x1=0\n",
    "\n",
    "                        poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                        total_poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "                    Rep2=trips_df[\"rep2\"]\n",
    "                    for rep2 in Rep2:\n",
    "                        if rep2>=1:\n",
    "                            x2=rep2-1\n",
    "                        else:\n",
    "                            x2=0\n",
    "\n",
    "                        poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                        total_poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "                    Rep3=trips_df[\"rep3\"]\n",
    "                    for rep3 in Rep3:\n",
    "                        if rep3>=1:\n",
    "                            x3=rep3-1\n",
    "                        else:\n",
    "                            x3=0\n",
    "\n",
    "                        poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                        total_poisson_Rep3_list.append(poisson3)\n",
    "\n",
    "    #With the data gathered above, new columns are created with the information.\n",
    "    df[\"NetSFC_per_million\"]=total_NetSFC\n",
    "    df[\"SI\"]=total_SI_list\n",
    "    df[\"Poisson_Rep1\"]=total_poisson_Rep1_list\n",
    "    df[\"Poisson_Rep2\"]=total_poisson_Rep2_list\n",
    "    df[\"Poisson_Rep3\"]=total_poisson_Rep3_list\n",
    "    statistics_Total_dfs_list.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "#This box runs several equations and creates multiple columns.\n",
    "\n",
    "#A lot of what is happening is similar to box 13, so I only explain the new parts.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:  \n",
    "    statistics_single_dfs_list=[]\n",
    "    for df in new_single_dfs_list:\n",
    "        single_NetSFC=[]\n",
    "        single_SI_list=[]\n",
    "        single_poisson_Rep1_list=[]\n",
    "        single_poisson_Rep2_list=[]\n",
    "        single_poisson_Rep3_list=[]\n",
    "\n",
    "        plate_list=df[\"Plate_ID\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "\n",
    "            read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "            for read_date in read_date_list:\n",
    "                read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "                donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                for donor in donor_num_list:\n",
    "                    donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "                    #Using the read date DF, the negative averages will be made.\n",
    "                    Neg_df=donornum_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "\n",
    "                    Neg_Avg=float(Neg_df[\"Average\"])\n",
    "\n",
    "                    #If the neg average is above or equal to 2, mu for poisson will use it.\n",
    "                    if Neg_Avg>=2:\n",
    "                        mu=Neg_Avg\n",
    "\n",
    "                    #Otherwise mu will be defined as 2.\n",
    "                    else:\n",
    "                        mu=2\n",
    "\n",
    "                    trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                    for trip in trip_list:\n",
    "                        trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                        Average_list=trips_df[\"Average\"]\n",
    "                        for average in Average_list:\n",
    "\n",
    "                            #This runs the netsfu function then uses it to make the SI\n",
    "                            netsfc=get_netsfu(average)\n",
    "                            single_NetSFC.append(netsfc)\n",
    "\n",
    "                            #As long as the Neg average is above 0, the average of each triplicate will be divided by it.\n",
    "                            if Neg_Avg > 0:\n",
    "                                SI=(average/Neg_Avg)\n",
    "                                single_SI_list.append(SI)\n",
    "\n",
    "                            #Otherwise SI is just the average of the triplicate.\n",
    "                            else:\n",
    "                                zero_neg_SI=average\n",
    "                                single_SI_list.append(zero_neg_SI)\n",
    "\n",
    "                        Rep1=trips_df[\"rep1\"]\n",
    "                        #The next for loops are running poisson.\n",
    "                        for rep1 in Rep1:\n",
    "                            if rep1>=1:\n",
    "                                x1=rep1-1\n",
    "                            else:\n",
    "                                x1=0\n",
    "\n",
    "                            poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                            single_poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "                        Rep2=trips_df[\"rep2\"]\n",
    "                        for rep2 in Rep2:\n",
    "                            if rep2>=1:\n",
    "                                x2=rep2-1\n",
    "                            else:\n",
    "                                x2=0\n",
    "\n",
    "                            poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                            single_poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "                        Rep3=trips_df[\"rep3\"]\n",
    "                        for rep3 in Rep3:\n",
    "                            if rep3>=1:\n",
    "                                x3=rep3-1\n",
    "                            else:\n",
    "                                x3=0\n",
    "\n",
    "                            poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                            single_poisson_Rep3_list.append(poisson3)\n",
    "\n",
    "        #With the data gathered above, new columns are created with the information.\n",
    "        df[\"NetSFC_per_million\"]=single_NetSFC\n",
    "        df[\"SI\"]=single_SI_list\n",
    "        df[\"Poisson_Rep1\"]=single_poisson_Rep1_list\n",
    "        df[\"Poisson_Rep2\"]=single_poisson_Rep2_list\n",
    "        df[\"Poisson_Rep3\"]=single_poisson_Rep3_list\n",
    "        statistics_single_dfs_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#31\n",
    "#This little function is used to make the ttest result reflect a one tailed result instead of a two tailed one.\n",
    "def get_pvalue(self):\n",
    "        \n",
    "        return (self[1]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32\n",
    "#Most of this box you've seen in prior boxes.\n",
    "tt_total_dfs_list=[]\n",
    "for df in statistics_Total_dfs_list:   \n",
    "    \n",
    "    total_ttests=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "\n",
    "                Neg_triplicates=[]\n",
    "                Neg_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==\"neg\")]\n",
    "                Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "                    triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "                    #This will get the Ttest p-value from each triplicate\n",
    "                    ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "                    total_ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "    df[\"Ttest_pvalue\"]=total_ttests\n",
    "    tt_total_dfs_list.append(df)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#33\n",
    "#Most of this box you've seen in prior boxes.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else: \n",
    "    tt_single_dfs_list=[]\n",
    "    for df in statistics_single_dfs_list:   \n",
    "\n",
    "        single_ttests=[]\n",
    "\n",
    "        plate_list=df[\"Plate_ID\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "\n",
    "            read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "            for read_date in read_date_list:\n",
    "                read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "                \n",
    "                donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                for donor in donor_num_list:\n",
    "                    donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "\n",
    "                    Neg_triplicates=[]\n",
    "                    Neg_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==\"neg\")]\n",
    "                    Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "                    trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                    for trip in trip_list:\n",
    "                        trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "                        triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "                        #This will get the Ttest p-value from each triplicate\n",
    "                        ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "                        single_ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "        df[\"Ttest_pvalue\"]=single_ttests\n",
    "        tt_single_dfs_list.append(df)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34\n",
    "#Another import of stats here, its odd but I found it to be necessary.\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#35\n",
    "#This box creates a new column that automatically checks if the triplicate is a positive hit based on specifications defined within.\n",
    "\n",
    "#These specifications can be tinkered with, but it is sort of finicky.\n",
    "positive_total_dfs_list=[]\n",
    "for df in statistics_Total_dfs_list:   \n",
    "\n",
    "    total_positive=[]\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "                \n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "\n",
    "                        #This will check the sfc and if its over the cutoff you defined, it will move to the next step. Otherwise it will be labelled negative.\n",
    "                        if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                            for si in trips_df[\"SI\"]:\n",
    "\n",
    "                                #If the triplicate's si is greater than 2, it will move to the next step. Otherwise it will be labelled as negative.\n",
    "                                if si>=2:\n",
    "\n",
    "                                    #These next few steps will create an average for the poissons.\n",
    "                                    rep_list=[]\n",
    "                                    for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                        r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r1)\n",
    "                                    for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                        r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r2)\n",
    "                                    for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                        r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r3)\n",
    "                                    Poisson_avg=statistics.mean(rep_list)\n",
    "\n",
    "                                    #Now the pvalue cutoffs will be checked.\n",
    "                                    for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "\n",
    "                                        #If the pvalue of the ttest is less than .05 OR the poisson avg found above is less than 5, the sfc # will be put into the new positive column.\n",
    "                                        if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                            t=sfc\n",
    "                                            total_positive.append(t)\n",
    "\n",
    "                                        #Otherwise, it will be labelled negative in the positive column.\n",
    "                                        else:\n",
    "                                            f=\"Negative\"\n",
    "                                            total_positive.append(f)\n",
    "                                else:\n",
    "                                    f=\"Negative\"\n",
    "                                    total_positive.append(f)\n",
    "                        else:\n",
    "                            f=\"Negative\"\n",
    "                            total_positive.append(f)\n",
    "\n",
    "    df[\"Positive\"]=total_positive\n",
    "    positive_total_dfs_list.append(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#36\n",
    "#This box creates a new column that automatically checks if the triplicate is a positive hit based on specifications defined within.\n",
    "\n",
    "#These specifications can be tinkered with, but it is sort of finicky.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else: \n",
    "    positive_single_dfs_list=[]\n",
    "    for df in statistics_single_dfs_list:   \n",
    "\n",
    "        single_positive=[]\n",
    "        plate_list=df[\"Plate_ID\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "\n",
    "            read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "            for read_date in read_date_list:\n",
    "                read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "                donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "                for donor in donor_num_list:\n",
    "                    donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "\n",
    "                    trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                    for trip in trip_list:\n",
    "                        trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                        for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "\n",
    "                            #This will check the sfc and if its over the cutoff you defined, it will move to the next step. Otherwise it will be labelled negative.\n",
    "                            if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                                for si in trips_df[\"SI\"]:\n",
    "\n",
    "                                    #If the triplicate's si is greater than 2, it will move to the next step. Otherwise it will be labelled as negative.\n",
    "                                    if si>=2:\n",
    "\n",
    "                                        #These next few steps will create an average for the poissons.\n",
    "                                        rep_list=[]\n",
    "                                        for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                            r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                            rep_list.append(r1)\n",
    "                                        for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                            r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                            rep_list.append(r2)\n",
    "                                        for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                            r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                            rep_list.append(r3)\n",
    "                                        Poisson_avg=statistics.mean(rep_list)\n",
    "\n",
    "                                        #Now the pvalue cutoffs will be checked.\n",
    "                                        for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "\n",
    "                                            #If the pvalue of the ttest is less than .05 OR the poisson avg found above is less than 5, the sfc # will be put into the new positive column.\n",
    "                                            if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                                t=sfc\n",
    "                                                single_positive.append(t)\n",
    "\n",
    "                                            #Otherwise, it will be labelled negative in the positive column.\n",
    "                                            else:\n",
    "                                                f=\"Negative\"\n",
    "                                                single_positive.append(f)\n",
    "                                    else:\n",
    "                                        f=\"Negative\"\n",
    "                                        single_positive.append(f)\n",
    "                            else:\n",
    "                                f=\"Negative\"\n",
    "                                single_positive.append(f)\n",
    "\n",
    "        df[\"Positive\"]=single_positive\n",
    "        positive_single_dfs_list.append(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37\n",
    "\n",
    "#The final master df is made so that each LED Total DF is put next to eachother.\n",
    "totals_headers=[\"Plate_ID\",\"Read_Date\",\"Donor_number\",\"Triplicate_#\"]\n",
    "\n",
    "#Magic Line!\n",
    "master_totals_combined_df=reduce(lambda x,y : pd.merge(x, y, on = totals_headers), positive_total_dfs_list)\n",
    "\n",
    "#This saves the master Df to the new master excel file that will appear later.\n",
    "master_totals_combined_df.to_excel(writer, sheet_name=\"Master_Totals_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#38\n",
    "#The final master df is made so that each LED Total DF is put next to eachother.\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    single_headers=[\"Plate_ID\",\"Read_Date\",\"Donor_number\",\"Triplicate_#\"]\n",
    "\n",
    "    #Magic Line!\n",
    "    master_singles_combined_df=reduce(lambda x,y : pd.merge(x, y, on = single_headers), positive_single_dfs_list)\n",
    "\n",
    "    #This saves the master Df to the new master excel file that will appear later.\n",
    "    master_singles_combined_df.to_excel(writer, sheet_name=\"Master_Singles_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#39\n",
    "New_combined_col=[\"Plate_ID\", \"Read_Date\", \"Donor_number\", \"Triplicate_#\"]\n",
    "\n",
    "rep_str=\"rep\"\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "for i in range(0, (len(rep_list))):\n",
    "    New_combined_col.append(\"rep\"+str(i+1))\n",
    "\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    new_singles_and_combined_df_list=[]\n",
    "\n",
    "    plate_list1=All_Activation_Summary[\"Plate\"].unique()\n",
    "    for plate in plate_list1:\n",
    "        plate_df = All_Activation_Summary.loc[(All_Activation_Summary['Plate']==plate)]\n",
    "\n",
    "        Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "        for read_date in Read_date_list:\n",
    "            read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    #taking the unique triplet and joining them together into a list of unique rows\n",
    "                    new_rows = [ plate, read_date, donor, trip ]\n",
    "\n",
    "                    #what is left after the loops above is a unique set of 3 or 6 SFUs per donor, plate, and pool\n",
    "                    #this will now be ran through iterrows, which will isolate each SFU by row and index\n",
    "                    for index, row in trips_df.iterrows():\n",
    "\n",
    "                        #with these isolated SFUs, a new object is formed\n",
    "                        SFU=row[\"Sum_of_all_activation\"]\n",
    "\n",
    "                        #this object is now added to the new row set made above\n",
    "                        new_rows.append( SFU )\n",
    "\n",
    "                        #the new row can now be added into a list to become a new df\n",
    "                \n",
    "                    new_singles_and_combined_df_list.append(new_rows)\n",
    "\n",
    "    #this is taking the new rows and making them into a new df\n",
    "    new_singles_and_combined_df=pd.DataFrame( new_singles_and_combined_df_list , columns = New_combined_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    new_singles_and_combined_df_Average=new_singles_and_combined_df.mean(axis=1, skipna=True)\n",
    "    new_singles_and_combined_df_StDev=new_singles_and_combined_df.std(axis=1, skipna=True)\n",
    "\n",
    "    new_singles_and_combined_df[\"Average\"]=new_singles_and_combined_df_Average\n",
    "    new_singles_and_combined_df[\"StDev\"]=new_singles_and_combined_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#41\n",
    "NetSFC=[]\n",
    "SI_list=[]\n",
    "poisson_Rep1_list=[]\n",
    "poisson_Rep2_list=[]\n",
    "poisson_Rep3_list=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    plate_list2=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list2:\n",
    "        plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "\n",
    "\n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "           \n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "                \n",
    "                Neg_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==\"neg\")]\n",
    "                Neg_Avg=float(Neg_df[\"Average\"])\n",
    "\n",
    "                if Neg_Avg>=2:\n",
    "                    mu=Neg_Avg\n",
    "                else:\n",
    "                    mu=2\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "\n",
    "                    Average_list=trips_df[\"Average\"]\n",
    "                    for average in Average_list:\n",
    "\n",
    "                        #taking each average minus the negative average of the pool then multiplying it by 5\n",
    "                        netsfc=get_netsfu(average)\n",
    "                        NetSFC.append(netsfc)\n",
    "\n",
    "                        if Neg_Avg > 0:\n",
    "                            SI=(average/Neg_Avg)\n",
    "                            SI_list.append(SI)\n",
    "                        else:\n",
    "                            zero_neg_SI=average\n",
    "                            SI_list.append(zero_neg_SI)\n",
    "\n",
    "                    Rep1=trips_df[\"rep1\"]\n",
    "                    for rep1 in Rep1:\n",
    "                        if rep1>=1:\n",
    "                            x1=rep1-1\n",
    "                        else:\n",
    "                            x1=0\n",
    "\n",
    "                        poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                        poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "                    Rep2=trips_df[\"rep2\"]\n",
    "                    for rep2 in Rep2:\n",
    "                        if rep2>=1:\n",
    "                            x2=rep2-1\n",
    "                        else:\n",
    "                            x2=0\n",
    "\n",
    "                        poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                        poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "                    Rep3=trips_df[\"rep3\"]\n",
    "                    for rep3 in Rep3:\n",
    "                        if rep3>=1:\n",
    "                            x3=rep3-1\n",
    "                        else:\n",
    "                            x3=0\n",
    "\n",
    "                        poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                        poisson_Rep3_list.append(poisson3)\n",
    "\n",
    "    new_singles_and_combined_df[\"NetSFC_per_million\"]=NetSFC\n",
    "    new_singles_and_combined_df[\"SI\"]=SI_list\n",
    "    new_singles_and_combined_df[\"Poisson_Rep1\"]=poisson_Rep1_list\n",
    "    new_singles_and_combined_df[\"Poisson_Rep2\"]=poisson_Rep2_list\n",
    "    new_singles_and_combined_df[\"Poisson_Rep3\"]=poisson_Rep3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#42\n",
    "ttests=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    plate_list3=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list3:\n",
    "        plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "\n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "            \n",
    "                Neg_triplicates=[]\n",
    "                Neg_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==\"neg\")]\n",
    "                Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "                    triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "\n",
    "                    ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "                    ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "    new_singles_and_combined_df[\"Ttest_pvalue\"]=ttests        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#43\n",
    "new_singles_and_combined_positive=[]\n",
    "if len(LEDcombinations) <2:\n",
    "    pass\n",
    "else:\n",
    "    plate_list4=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list4:\n",
    "        plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "\n",
    "\n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "            donor_num_list=read_date_df[\"Donor_number\"].unique()\n",
    "            for donor in donor_num_list:\n",
    "                donornum_df=read_date_df.loc[(read_date_df[\"Donor_number\"]==donor)]\n",
    "            \n",
    "                #repeating now for the unique pool responses in the plate df\n",
    "                trip_list=donornum_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=donornum_df.loc[(donornum_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "                        if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                            for si in trips_df[\"SI\"]:\n",
    "                                if si>=2:\n",
    "                                    rep_list=[]\n",
    "                                    for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                        r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r1)\n",
    "                                    for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                        r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r2)\n",
    "                                    for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                        r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                        rep_list.append(r3)\n",
    "                                    Poisson_avg=statistics.mean(rep_list)\n",
    "                                    for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "                                        if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                            t=sfc\n",
    "                                            new_singles_and_combined_positive.append(t)\n",
    "                                        else:\n",
    "                                            f=\"Negative\"\n",
    "                                            new_singles_and_combined_positive.append(f)\n",
    "                                else:\n",
    "                                    f=\"Negative\"\n",
    "                                    new_singles_and_combined_positive.append(f)\n",
    "                        else:\n",
    "                            f=\"Negative\"\n",
    "                            new_singles_and_combined_positive.append(f)\n",
    "\n",
    "    new_singles_and_combined_df[\"Positive\"]=new_singles_and_combined_positive\n",
    "    new_singles_and_combined_df.to_excel(writer, sheet_name=\"Master_Singles+Combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44\n",
    "#This saves the Master Excel file to path defined in the writer function.\n",
    "#If unchanged, it will end up on your desktop.\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
