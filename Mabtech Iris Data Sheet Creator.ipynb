{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello and welcome to the Mabtech Iris Data Sheet Creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made this script to work specifically for the Mabtech Iris machine's folder output and for a particular plate layout. The script is based on particular parameters, so please refer to the included Word document if you are unsure whether your plate will work. At the end of this script you should have a Master Excel file that summarizes the data within each folder. The script currently takes in the triplicates and produces an average, standard deviation, SFC per million, SI, Poisson per well in triplicate, p-value (based on T test), and determines significance based on these. If any errors come up that the script does not address, please let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this script to work correctly, please read and follow the instructions following the #s in box #2. Once you're ready to run a box, make sure it is selected (has a green or blue box around it) then hit the \"Run\" button found above. If an * appears in place of a number within the brackets to the left of the box, the script within the box is running, and if a number appears, that means it finished. Once one box is done, move on to the next box and hit \"Run.\" You will notice other #s within each box, these are explanations of what each line of code is doing. When you're ready, select box #1 and hit run, then move on to box #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#Always run this box first.\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#This is the most complicated box, but it is necessary.\n",
    "#Please fill in the below variables, all that is needed for you to change will be on the RIGHT side of equals sign.\n",
    "#Once filled in, run this box.\n",
    "\n",
    "#a\n",
    "#Create the path to wherever your Iris folders or Iris excel sheets are. \n",
    "#Pay close attention to capitalization and spaces.\n",
    "#Use / to indicate the next step in the path.\n",
    "#The starting location of the path is the next step after the location of this program.\n",
    "#So if you dragged this program, \"Mabtech Iris Data Sheet Creator,\" to your Desktop, Desktop WOULD NOT be used below.\n",
    "#If you dragged this program into a folder, say a master folder that contains other folders of data, that folder name WOULD NOT be included below.\n",
    "#The example below starts in the desktop (because the script is not housed in Desktop), goes into a folder labelled Public Py Folder, then ends in the folder Folder with folders which contains all the iris output folders.\n",
    "\n",
    "folder_path = \"TB/TB Results\"\n",
    "\n",
    "#b\n",
    "#Fill in the wells that your negative control is in.\n",
    "#Stay within the quotes for the Neg_list, but DO NOT use quotes for the Number_of_negative_wells (also, use numbers, don't spell out number).\n",
    "#Add quotes and wells within the list based on the number you used.\n",
    "#So in the example below, 6 wells were used as negatives, and the list has 6 quotes well locations.\n",
    "\n",
    "\n",
    "Number_of_negative_wells = 6\n",
    "\n",
    "Neg_list = [\"H10\",\"H11\",\"H12\",\"G10\",\"G11\",\"G12\"]\n",
    "\n",
    "#c\n",
    "#Fill in the number of cells you plated per well using numbers (don't spell out number).\n",
    "#DO NOT use quotes.\n",
    "\n",
    "Number_of_cells_per_well = 200000\n",
    "\n",
    "#d\n",
    "#Fill in the minimum number of sfc hits acceptable to be considered a positive hit.\n",
    "#In the example below, positive triplicates are only considered significant in the last column if their net sfcs are above 20, otherwise it is considered negative.\n",
    "\n",
    "Number_of_sfcs_necessary_to_be_considered=20\n",
    "\n",
    "#e\n",
    "#This is the blank master excel sheet that will be filled out through the script.\n",
    "#Fill in the location (same way as path above was filled in) and name what you want the end excel file to be.\n",
    "#Only fill in what is between the first quotation marks, do not change the engine.\n",
    "#If this program is located on your desktop and you want it to end there, just create a name for the excel file within the quotes.\n",
    "#Make sure to END WITH .xlsx to your title name, otherwise it will not appear.\n",
    "\n",
    "writer = pd.ExcelWriter('pandas_master_excel.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "#This creates a directory of the iris folders using the path made above.\n",
    "Mabtech_Iris_Folders_dir = os.listdir(folder_path) \n",
    "\n",
    "#This empty list will be filled with your data.\n",
    "df_list = []\n",
    "\n",
    "#This begins a loop searchig through the folder you labelled in the path.\n",
    "for option in Mabtech_Iris_Folders_dir:\n",
    "   \n",
    "    #This is looking at the folders/files in the path and checking for excel files, if it finds them it will continue to the next step.\n",
    "    if option.endswith(\".xlsx\"):\n",
    "        \n",
    "        #If excel files are found, it will create a DataFrame with the data found on sheet 2 of the mabtech excel files.\n",
    "        path_new = folder_path + \"/\" + option\n",
    "        Excel_Data=pd.read_excel(path_new,sheet_name=1, usecols=\"A:H\")\n",
    "        \n",
    "        #This adds the Dataframe to the empty list.\n",
    "        df_list.append(Excel_Data)\n",
    "    \n",
    "    #When an excel starts with $, it is often corrupted. This checks and creates a print statement below if it sees a possible corrupt file.\n",
    "        if option.startswith(\"$\"):\n",
    "            print(\"The following file may be broken or corrupted \" + option)\n",
    "   \n",
    "    #If no excel files are found, this will go into the folders within the path and look for excel files.\n",
    "    else:\n",
    "        \n",
    "        #This avoids hidden files to prevent them from breaking the code.\n",
    "        if not option.startswith(\".\"):\n",
    "            excel_files_in_folder = []\n",
    "            for file in os.listdir(folder_path+\"/\"+option):\n",
    "                if file.endswith(\".xlsx\"):\n",
    "                    \n",
    "                    #After going into the next folder looking for the excel file, it will create a Dataframe from the found excel.\n",
    "                    excel_files_in_folder.append(file)\n",
    "                    path_new = folder_path + \"/\" + option + \"/\" + file\n",
    "                    Excel_Data=pd.read_excel(path_new,sheet_name=1, usecols=\"A:H\")\n",
    "                    \n",
    "                    #This adds the Dataframe to the empty list\n",
    "                    df_list.append(Excel_Data)\n",
    "                    \n",
    "                    #When an excel starts with $, it is often corrupted. This checks and creates a print statement below if it sees a possible corrupt file.\n",
    "                    if file.startswith(\"$\"):\n",
    "                        print(\"Check the following folder(s) for broken excel files \" + option)\n",
    "                    \n",
    "            #Because Mabtech Iris Folders only contain one excel per output, I incorporated this check to ensure only one excel file appears within the folder.\n",
    "            if len(excel_files_in_folder) != 1:\n",
    "                print(\"Check the following folder(s) for duplicate excel files: \" + option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "#This box defines a function that creates a list of headers within a dataframe list.\n",
    "#It will be used as a simple check to make sure the correct headers were read in to a dataframe.\n",
    "def check_headers(df_list):\n",
    "    Col_Names=[]\n",
    "    \n",
    "    #Start of a for loop that will look through the newly created list.\n",
    "    for dataframes in df_list:\n",
    "        \n",
    "        #This creates an object, x, which contains the list of column headers.\n",
    "        x=list(dataframes.columns)\n",
    "        Col_Names.append(x)\n",
    "        \n",
    "    #This ends the function and tells it what to output when it is ran (in this case the list of headers).    \n",
    "    return(Col_Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "#This function is needed to run unequal columns together.\n",
    "#If you have multiple Negative triplicates, this will ensure the longer row wont cause a break in the script.\n",
    "def blank_filler(new_df_row):\n",
    "    \n",
    "    #This next section creates a loop that fills in blanks (NaN) until the new df row is equal to the New_col list.\n",
    "    while ( len(new_df_row) < len( New_col) ):\n",
    "        \n",
    "        #The word None is used here to fill in NaN variables that python will recognize as null data rather than a 0.\n",
    "        #This is imprtant bc if anything else is used (string-wise) the column will no longer be seen as a data column by python.\n",
    "        new_df_row.append(None)\n",
    "        \n",
    "    return (new_df_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#This creates a basic Dataframe that includes triplicates based on the well.\n",
    "\n",
    "#This creates an empty dictionary which will be used to associate triplicates to wells.\n",
    "letter_dict = {}\n",
    "letter_str='ABCDEFGH'\n",
    "\n",
    "#This for loop associates individual letters to a number that will be used to call on its location.\n",
    "for i in range(0,len(letter_str)):\n",
    "    letter = letter_str[i]\n",
    "    letter_dict[i]=letter\n",
    "\n",
    "well_list=[]\n",
    "trip_marker_list=[]\n",
    "\n",
    "#This for loop starts to number the 96 well plate.\n",
    "for well_number in range(0,96):    \n",
    "    col_number = (well_number % 12) + 1\n",
    "\n",
    "    row_index = int(well_number/12)\n",
    "    row_letter = letter_dict[row_index]\n",
    "\n",
    "    #Calculate triplicates as if numbered across the plate.\n",
    "    trip_across = int(well_number/3) + 1\n",
    "    row_offset = -(row_index*3)\n",
    "    col_offset = int((col_number-1)/3)*7\n",
    "\n",
    "    #Add row & column offsets to the triplicate value (across) to find triplicate value when going down the plate.\n",
    "    trip_down = str(trip_across + row_offset + col_offset)\n",
    "    \n",
    "    #The well now matches the Well output from Iris.\n",
    "    well=row_letter+ str(col_number)\n",
    "\n",
    "    well_list.append(well)\n",
    "    \n",
    "    #This last for loop appends the appropriate triplicate number based on the well its associated with and if it is a negative control.\n",
    "    if well in Neg_list:\n",
    "        trip_marker_list.append(\"neg\")\n",
    "    else:\n",
    "        trip_marker_list.append(trip_down)\n",
    "\n",
    "#This is the DF that will be used to create a triplicate column in the future Dfs\n",
    "trip_df_marker=pd.DataFrame()\n",
    "trip_df_marker[\"Well\"]=well_list\n",
    "trip_df_marker[\"Triplicate_#\"]=trip_marker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "#This list is what each column header should be when iris creates the file.\n",
    "Expected_Cols=[\"Plate\", \"Well\",\"Read Date\", \"Saved Date\", \"Machine ID\", \"Analyte Secreting Population\", \"LED Filter\", \"Spot Forming Units (SFU)\"]\n",
    "\n",
    "#This object is the datafile list after the function check_headers has been run.\n",
    "Col_headers=check_headers(df_list)\n",
    "\n",
    "#This for loop checks if each header within the excel files are what should be expected from the iris output.\n",
    "for col_head in Col_headers:\n",
    "    \n",
    "    #If the headers are other than what is expected, there is an excel file that is not an iris output or is a broken iris excel file.\n",
    "    assert col_head == Expected_Cols, \"Column headers within one or more excel files are not as expected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LED490 Total', 'LED490 Single', 'LED550 Total', 'LED550 Single', 'LED640 Total', 'LED640 Single', 'LED490+LED550', 'LED490+LED640', 'LED550+LED640', 'LED490+LED550+LED640']\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "#Combines all dataframes made above into one large dataframe and removes the three columns below.\n",
    "complete_df=(pd.concat(df_list, axis=0)).drop(columns=[\"Machine ID\",\"Saved Date\",\"Analyte Secreting Population\"])\n",
    "  \n",
    "LEDcombinations=[]\n",
    "\n",
    "#This isolates the different LED filters that the Mabtech puts out.\n",
    "#This will be used to ensure that no matter how many Fluorospot colors you used (double, triple, etc.), the following script will gather all the data for them.\n",
    "LED_list=complete_df[\"LED Filter\"].unique()\n",
    "for LED in LED_list:\n",
    "    LEDcombinations.append(LED)\n",
    "\n",
    "#This prints out the list of unique found combinations found within the LED Filter column of the Mabtech excel output.\n",
    "#If it is not what you expected, please contact me.\n",
    "print(LEDcombinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "\n",
    "#Each of these empty lists will be filled based on the specific LED Filter only.\n",
    "#Though only the Total df list and Single combo df list will be used in the rest of the script, I parcel out the singles and combo into their own list in case theyre needed.\n",
    "Total_dfs_list=[]\n",
    "Singles_list=[]\n",
    "Combo_dfs_list=[]\n",
    "Single_combo_dfs_list=[]\n",
    "\n",
    "#This defines an object headers, which will be used later.\n",
    "headers=[\"Plate\", \"Well\",\"Read Date\",\"LED Filter\"]\n",
    "\n",
    "#This for loop is using the unique list defined in the previous box to ensure all the different combinations are incorporated.\n",
    "for led in LEDcombinations:\n",
    "    \n",
    "    #The totals will be caught in this loop then added to their appropriate list.\n",
    "    if led.endswith(\"Total\"):\n",
    "        total_df = (complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        Total_dfs_list.append(total_df)\n",
    "    \n",
    "    #The combinations will be caught in this loop then added to their appropriate lists.\n",
    "    if \"+\" in led:\n",
    "        \n",
    "        #This both defines the df by its LED filter and renames the SFU column to its LED filter (useful later).\n",
    "        combo_df=(complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        combo_df_for_combined_df = ((complete_df.loc[complete_df[\"LED Filter\"]==led]).rename(columns={\"Spot Forming Units (SFU)\": led}))\n",
    "        Combo_dfs_list.append(combo_df)\n",
    "        Single_combo_dfs_list.append(combo_df_for_combined_df)\n",
    "    \n",
    "    #The singles will be caught in this loop then added to their appropriate lists.\n",
    "    if led.endswith(\"Single\"):\n",
    "        \n",
    "        #This both defines the df by its LED filter and renames the SFU column to its LED filter (useful later).\n",
    "        single_df=(complete_df.loc[complete_df[\"LED Filter\"]==led])\n",
    "        singles_for_combined_df=((complete_df.loc[complete_df[\"LED Filter\"]==led]).rename(columns={\"Spot Forming Units (SFU)\": led}))\n",
    "        Singles_list.append(single_df)\n",
    "        Single_combo_dfs_list.append(singles_for_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "#This box containes a bit of magic I dont fully comprehend, but it combines dataframes based on their headers and LED filters.\n",
    "from functools import reduce\n",
    "single_combo_header=[\"Plate\",\"Well\",\"Read Date\"]\n",
    "New_single_combo_dfs_list=[]\n",
    "\n",
    "for df in Single_combo_dfs_list:\n",
    "    new_df=df.drop(columns=\"LED Filter\")\n",
    "    New_single_combo_dfs_list.append(new_df)\n",
    "    \n",
    "#The magic line is below. The merge will combine the DFs, but lambda and reduce remove repetitive columns to make the DF cleaner\n",
    "singles_and_combined_df=reduce(lambda x,y : pd.merge(x, y, on = single_combo_header), New_single_combo_dfs_list)\n",
    "\n",
    "#This sums up the singles and columns into a new sum row.\n",
    "sum_column=singles_and_combined_df.sum(axis=1)\n",
    "\n",
    "singles_and_combined_df[\"Sum_of_all_activation\"]=sum_column\n",
    "\n",
    "#This runs the triplicate function on the newly created DF.\n",
    "triplicate_singles_and_combined_df=pd.merge(singles_and_combined_df,trip_df_marker,how=\"outer\",on=(\"Well\"))\n",
    "\n",
    "All_Activation_Summary=triplicate_singles_and_combined_df[[\"Plate\",\"Read Date\",\"Well\",\"Triplicate_#\",\"LED490 Single\",\"LED550 Single\",\"LED640 Single\",\"LED550+LED640\",\"LED490+LED640\",\"LED490+LED550\",\"LED490+LED550+LED640\",\"Sum_of_all_activation\"]]\n",
    "\n",
    "#Now that the Df is made and the new columns are added, this saves it to the master excel sheet.\n",
    "All_Activation_Summary.to_excel(writer,sheet_name=\"All_Activation_Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 This step is repeated three times for three different lists of DFs.\n",
    "\n",
    "#A new DF is created based on the triplicates that get assigned by the function created in box #6.\n",
    "trip_Total_dfs_list=[]\n",
    "for df in Total_dfs_list:\n",
    "    new_df=pd.merge(df,trip_df_marker,how=\"outer\",on=(\"Well\"))\n",
    "    trip_Total_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "trip_single_dfs_list=[]\n",
    "for df in Singles_list:\n",
    "    new_df=pd.merge(df,trip_df_marker,how=\"outer\",on=(\"Well\"))\n",
    "    trip_single_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "trip_combo_dfs_list=[]\n",
    "for df in Combo_dfs_list:\n",
    "    new_df=pd.merge(df,trip_df_marker,how=\"outer\",on=(\"Well\"))\n",
    "    trip_combo_dfs_list.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rep1', 'rep2', 'rep3', 'rep4', 'rep5', 'rep6']\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "#This is creating headers based on the number of negative wells you defined in box 2.\n",
    "rep_str=\"rep\"\n",
    "\n",
    "#This makes a list of the word \"rep\" for every negative well you defined.\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "num_rep_list=[]\n",
    "\n",
    "#This for loop will add numbers next to each rep in the list above.\n",
    "for i in range(0, (len(rep_list))):\n",
    "    num_rep_list.append(\"rep\"+str(i+1))\n",
    "print(num_rep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "#This function will be used to isolate specific datum in future triplicates.\n",
    "def get_replicates_from_row(df):\n",
    "    replicates=[]\n",
    "    for replicate_header in num_rep_list:\n",
    "        for sfu in df[replicate_header]:\n",
    "            replicates.append(sfu)\n",
    "        \n",
    "    return(replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 This step gets repeated three times to three different lists of DFs\n",
    "#This is a new a list of new col headers which will be used for a new df.\n",
    "New_col=[\"Plate_ID\", \"Read_Date\", \"Triplicate_#\",\"LED_Filter\"]\n",
    "\n",
    "#This is a repeat of the rep headers, but it adds it directly to the new headers above\n",
    "rep_str=\"rep\"\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "for i in range(0, (len(rep_list))):\n",
    "    New_col.append(\"rep\"+str(i+1))\n",
    "\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_Total_dfs_list=[]\n",
    "\n",
    "#For loop within the list of total dataframes.\n",
    "for df in trip_Total_dfs_list:\n",
    "    new_total_df_list=[]\n",
    "    \n",
    "    #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "    LED_list=df[\"LED Filter\"].unique()\n",
    "    for ledfilter in LED_list:\n",
    "        ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "        \n",
    "        #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "        plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "            #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "            Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "            for read_date in Read_date_list:\n",
    "                read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "                #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                    new_rows = [ plate, read_date, trip, ledfilter ]\n",
    "\n",
    "                    #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                    #This for loop will now run through and isolate each SFU by row and name.\n",
    "                    for index, row in trips_df.iterrows():\n",
    "\n",
    "                        #With these isolated SFUs, a new object is formed.\n",
    "                        SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "\n",
    "                        #This object is now added to the new row set made above.\n",
    "                        new_rows.append( SFU )\n",
    "\n",
    "                    #The new row is then added into a list to become a new df.\n",
    "                    new_total_df_list.append(new_rows)\n",
    "\n",
    "        #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "        new_total_df=pd.DataFrame( new_total_df_list , columns = New_col )\n",
    "        \n",
    "        #This adds the new DF to a list of all the new Total Dfs.\n",
    "        new_Total_dfs_list.append(new_total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_single_dfs_list=[]\n",
    "\n",
    "#For loop within the list of total dataframes.\n",
    "for df in trip_single_dfs_list:\n",
    "    new_single_df_list=[]\n",
    "    \n",
    "    #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "    LED_list=df[\"LED Filter\"].unique()\n",
    "    for ledfilter in LED_list:\n",
    "        ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "        \n",
    "        #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "        plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "            #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "            Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "            for read_date in Read_date_list:\n",
    "                read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "                #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                    new_rows = [ plate, read_date, trip, ledfilter ]\n",
    "\n",
    "                    #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                    #This for loop will now run through and isolate each SFU by row and name.\n",
    "                    for index, row in trips_df.iterrows():\n",
    "\n",
    "                        #With these isolated SFUs, a new object is formed.\n",
    "                        SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "\n",
    "                        #This object is now added to the new row set made above.\n",
    "                        new_rows.append( SFU )\n",
    "\n",
    "                    #The new row is then added into a list to become a new df.\n",
    "                    new_single_df_list.append(new_rows)\n",
    "\n",
    "        #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "        new_single_df=pd.DataFrame( new_single_df_list , columns = New_col )\n",
    "        \n",
    "        #This adds the new DF to a list of all the new Total Dfs.\n",
    "        new_single_dfs_list.append(new_single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "\n",
    "#This empty list will be filled with the data beneath the new headers.\n",
    "new_combo_dfs_list=[]\n",
    "\n",
    "#For loop within the list of total dataframes.\n",
    "for df in trip_combo_dfs_list:\n",
    "    new_combo_df_list=[]\n",
    "    \n",
    "    #This isolates data based on LED Filter then creates a smaller DF with that data.\n",
    "    LED_list=df[\"LED Filter\"].unique()\n",
    "    for ledfilter in LED_list:\n",
    "        ledfilter_df=df.loc[(df[\"LED Filter\"]==ledfilter)]\n",
    "        \n",
    "        #This isolates data within the smaller DF based on Plate name then creates a smaller DF with that data.\n",
    "        plate_list=ledfilter_df[\"Plate\"].unique()\n",
    "        for plate in plate_list:\n",
    "            plate_df = df.loc[(df['Plate']==plate)]\n",
    "\n",
    "            #This isolates data within the even smaller DF based on Read date then creates a smaller DF with that data.\n",
    "            Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "            for read_date in Read_date_list:\n",
    "                read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "                #This isolates data within the read date DF based on Triplicate # then creates a smaller DF with that data.\n",
    "                trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "                for trip in trip_list:\n",
    "                    trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                    #Now the tiny DF will be segmented based on the pieces used to isolate the DF.\n",
    "                    new_rows = [ plate, read_date, trip, ledfilter ]\n",
    "\n",
    "                    #What is within the tiny DF is a unique set of 3 or 6 SFUs per LED, Plate, Read Date, Triplicate #.\n",
    "                    #This for loop will now run through and isolate each SFU by row and name.\n",
    "                    for index, row in trips_df.iterrows():\n",
    "\n",
    "                        #With these isolated SFUs, a new object is formed.\n",
    "                        SFU=row[\"Spot Forming Units (SFU)\"]\n",
    "\n",
    "                        #This object is now added to the new row set made above.\n",
    "                        new_rows.append( SFU )\n",
    "\n",
    "                    #The new row is then added into a list to become a new df.\n",
    "                    new_combo_df_list.append(new_rows)\n",
    "\n",
    "        #This is taking the list of new rows and making them into a new df and giving them the new headers defined int he beginning.\n",
    "        new_combo_df=pd.DataFrame( new_combo_df_list , columns = New_col )\n",
    "        \n",
    "        #This adds the new DF to a list of all the new Total Dfs.\n",
    "        new_combo_dfs_list.append(new_combo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19 This box is also repeated three times for three different lists of DFs.\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "for df in new_Total_dfs_list:\n",
    "    new_total_df_Average=df.mean(axis=1, skipna=True)\n",
    "    new_total_df_StDev=df.std(axis=1, skipna=True)\n",
    "    \n",
    "    df[\"Average\"]=new_total_df_Average\n",
    "    df[\"StDev\"]=new_total_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "for df in new_single_dfs_list:\n",
    "    new_single_df_Average=df.mean(axis=1, skipna=True)\n",
    "    new_single_df_StDev=df.std(axis=1, skipna=True)\n",
    "    \n",
    "    df[\"Average\"]=new_single_df_Average\n",
    "    df[\"StDev\"]=new_single_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "#This box takes the average and standard deviation of each triplicate then adds it to a new column in the DF.\n",
    "combo_header=[\"Plate_ID\",\"Read_Date\",\"Triplicate_#\"]\n",
    "for df in new_combo_dfs_list:\n",
    "    new_combo_df_Average=df.mean(axis=1, skipna=True)\n",
    "    new_combo_df_StDev=df.std(axis=1, skipna=True)\n",
    "    \n",
    "    df[\"Average\"]=new_combo_df_Average\n",
    "    df[\"StDev\"]=new_combo_df_StDev\n",
    "\n",
    "new_combo_df=reduce(lambda x,y : pd.merge(x, y, on = combo_header), new_combo_dfs_list)\n",
    "new_combo_df.to_excel(writer,sheet_name=\"Just_double_triple_producers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "#This essentially defines the poisson function to be called later.\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "#This essentially defines a function as stats to be called later.\n",
    "import math\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24\n",
    "#Using the number of cells you defined per plate earlier, this function creates the net sfu number from an average.\n",
    "def get_netsfu(average):\n",
    "    num_to_get_to_mil=1000000/Number_of_cells_per_well\n",
    "    net_sfu=((average-Neg_Avg)*num_to_get_to_mil)\n",
    "    if net_sfu > 0:\n",
    "        return(net_sfu)\n",
    "    if net_sfu<0:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "#This box runs several equations and creates multiple columns.\n",
    "#A lot of what is happening is similar to box 13, so I only explain the new parts.\n",
    "statistics_Total_dfs_list=[]\n",
    "for df in new_Total_dfs_list:\n",
    "    total_NetSFC=[]\n",
    "    total_SI_list=[]\n",
    "    total_poisson_Rep1_list=[]\n",
    "    total_poisson_Rep2_list=[]\n",
    "    total_poisson_Rep3_list=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            #Using the read date DF, the negative averages will be made.\n",
    "            Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "\n",
    "            Neg_Avg=float(Neg_df[\"Average\"])\n",
    "            \n",
    "            #If the neg average is above or equal to 2, mu for poisson will use it.\n",
    "            if Neg_Avg>=2:\n",
    "                mu=Neg_Avg\n",
    "            \n",
    "            #Otherwise mu will be defined as 2.\n",
    "            else:\n",
    "                mu=2\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "            \n",
    "                Average_list=trips_df[\"Average\"]\n",
    "                for average in Average_list:\n",
    "\n",
    "                    #This runs the netsfu function then uses it to make the SI\n",
    "                    netsfc=get_netsfu(average)\n",
    "                    total_NetSFC.append(netsfc)\n",
    "                    \n",
    "                    #As long as the Neg average is above 0, the average of each triplicate will be divided by it.\n",
    "                    if Neg_Avg > 0:\n",
    "                        SI=(average/Neg_Avg)\n",
    "                        total_SI_list.append(SI)\n",
    "                    \n",
    "                    #Otherwise SI is just the average of the triplicate.\n",
    "                    else:\n",
    "                        zero_neg_SI=average\n",
    "                        total_SI_list.append(zero_neg_SI)\n",
    "\n",
    "                Rep1=trips_df[\"rep1\"]\n",
    "                #The next for loops are running poisson.\n",
    "                for rep1 in Rep1:\n",
    "                    if rep1>=1:\n",
    "                        x1=rep1-1\n",
    "                    else:\n",
    "                        x1=0\n",
    "\n",
    "                    poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                    total_poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "                Rep2=trips_df[\"rep2\"]\n",
    "                for rep2 in Rep2:\n",
    "                    if rep2>=1:\n",
    "                        x2=rep2-1\n",
    "                    else:\n",
    "                        x2=0\n",
    "\n",
    "                    poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                    total_poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "                Rep3=trips_df[\"rep3\"]\n",
    "                for rep3 in Rep3:\n",
    "                    if rep3>=1:\n",
    "                        x3=rep3-1\n",
    "                    else:\n",
    "                        x3=0\n",
    "\n",
    "                    poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                    total_poisson_Rep3_list.append(poisson3)\n",
    "    \n",
    "    #With the data gathered above, new columns are created with the information.\n",
    "    df[\"NetSFC_per_million\"]=total_NetSFC\n",
    "    df[\"SI\"]=total_SI_list\n",
    "    df[\"Poisson_Rep1\"]=total_poisson_Rep1_list\n",
    "    df[\"Poisson_Rep2\"]=total_poisson_Rep2_list\n",
    "    df[\"Poisson_Rep3\"]=total_poisson_Rep3_list\n",
    "    statistics_Total_dfs_list.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26\n",
    "#This box runs several equations and creates multiple columns.\n",
    "#A lot of what is happening is similar to box 13, so I only explain the new parts.\n",
    "statistics_single_dfs_list=[]\n",
    "for df in new_single_dfs_list:\n",
    "    single_NetSFC=[]\n",
    "    single_SI_list=[]\n",
    "    single_poisson_Rep1_list=[]\n",
    "    single_poisson_Rep2_list=[]\n",
    "    single_poisson_Rep3_list=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            #Using the read date DF, the negative averages will be made.\n",
    "            Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "            Neg_Avg=float(Neg_df[\"Average\"])\n",
    "            \n",
    "            #If the neg average is above or equal to 2, mu for poisson will use it.\n",
    "            if Neg_Avg>=2:\n",
    "                mu=Neg_Avg\n",
    "            \n",
    "            #Otherwise mu will be defined as 2.\n",
    "            else:\n",
    "                mu=2\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "            \n",
    "                Average_list=trips_df[\"Average\"]\n",
    "                for average in Average_list:\n",
    "\n",
    "                    #This runs the netsfu function then uses it to make the SI\n",
    "                    netsfc=get_netsfu(average)\n",
    "                    single_NetSFC.append(netsfc)\n",
    "                    \n",
    "                    #As long as the Neg average is above 0, the average of each triplicate will be divided by it.\n",
    "                    if Neg_Avg > 0:\n",
    "                        SI=(average/Neg_Avg)\n",
    "                        single_SI_list.append(SI)\n",
    "                    \n",
    "                    #Otherwise SI is just the average of the triplicate.\n",
    "                    else:\n",
    "                        zero_neg_SI=average\n",
    "                        single_SI_list.append(zero_neg_SI)\n",
    "\n",
    "                Rep1=trips_df[\"rep1\"]\n",
    "                #The next for loops are running poisson.\n",
    "                for rep1 in Rep1:\n",
    "                    if rep1>=1:\n",
    "                        x1=rep1-1\n",
    "                    else:\n",
    "                        x1=0\n",
    "\n",
    "                    poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                    single_poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "                Rep2=trips_df[\"rep2\"]\n",
    "                for rep2 in Rep2:\n",
    "                    if rep2>=1:\n",
    "                        x2=rep2-1\n",
    "                    else:\n",
    "                        x2=0\n",
    "\n",
    "                    poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                    single_poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "                Rep3=trips_df[\"rep3\"]\n",
    "                for rep3 in Rep3:\n",
    "                    if rep3>=1:\n",
    "                        x3=rep3-1\n",
    "                    else:\n",
    "                        x3=0\n",
    "\n",
    "                    poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                    single_poisson_Rep3_list.append(poisson3)\n",
    "    \n",
    "    #With the data gathered above, new columns are created with the information.\n",
    "    df[\"NetSFC_per_million\"]=single_NetSFC\n",
    "    df[\"SI\"]=single_SI_list\n",
    "    df[\"Poisson_Rep1\"]=single_poisson_Rep1_list\n",
    "    df[\"Poisson_Rep2\"]=single_poisson_Rep2_list\n",
    "    df[\"Poisson_Rep3\"]=single_poisson_Rep3_list\n",
    "    statistics_single_dfs_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#27\n",
    "#This little function is used to make the ttest result reflect a one tailed result instead of a two tailed one.\n",
    "def get_pvalue(self):\n",
    "        \n",
    "        return (self[1]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28\n",
    "#Most of this box you've seen in prior boxes.\n",
    "tt_total_dfs_list=[]\n",
    "for df in statistics_Total_dfs_list:   \n",
    "    \n",
    "    total_ttests=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            Neg_triplicates=[]\n",
    "            Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "            Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "                triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "                #This will get the Ttest p-value from each triplicate\n",
    "                ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "                total_ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "    df[\"Ttest_pvalue\"]=total_ttests\n",
    "    tt_total_dfs_list.append(df)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "#Most of this box you've seen in prior boxes.\n",
    "tt_single_dfs_list=[]\n",
    "for df in statistics_single_dfs_list:   \n",
    "    \n",
    "    single_ttests=[]\n",
    "\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "            Neg_triplicates=[]\n",
    "            Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "            Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "                triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "                #This will get the Ttest p-value from each triplicate\n",
    "                ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "                single_ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "    df[\"Ttest_pvalue\"]=single_ttests\n",
    "    tt_single_dfs_list.append(df)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "#Another import of stats here, its odd but I found it to be necessary.\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#31\n",
    "#This box creates a new column that automatically checks if the triplicate is a positive hit based on specifications defined within.\n",
    "#These specifications can be tinkered with, but it is sort of finicky.\n",
    "positive_total_dfs_list=[]\n",
    "for df in statistics_Total_dfs_list:   \n",
    "\n",
    "    total_positive=[]\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "                    \n",
    "                    #This will check the sfc and if its over the cutoff you defined, it will move to the next step. Otherwise it will be labelled negative.\n",
    "                    if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                        for si in trips_df[\"SI\"]:\n",
    "                           \n",
    "                            #If the triplicate's si is greater than 2, it will move to the next step. Otherwise it will be labelled as negative.\n",
    "                            if si>=2:\n",
    "                                \n",
    "                                #These next few steps will create an average for the poissons.\n",
    "                                rep_list=[]\n",
    "                                for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                    r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r1)\n",
    "                                for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                    r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r2)\n",
    "                                for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                    r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r3)\n",
    "                                Poisson_avg=statistics.mean(rep_list)\n",
    "                                \n",
    "                                #Now the pvalue cutoffs will be checked.\n",
    "                                for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "                                    \n",
    "                                    #If the pvalue of the ttest is less than .05 OR the poisson avg found above is less than 5, the sfc # will be put into the new positive column.\n",
    "                                    if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                        t=sfc\n",
    "                                        total_positive.append(t)\n",
    "                                    \n",
    "                                    #Otherwise, it will be labelled negative in the positive column.\n",
    "                                    else:\n",
    "                                        f=\"Negative\"\n",
    "                                        total_positive.append(f)\n",
    "                            else:\n",
    "                                f=\"Negative\"\n",
    "                                total_positive.append(f)\n",
    "                    else:\n",
    "                        f=\"Negative\"\n",
    "                        total_positive.append(f)\n",
    "\n",
    "    df[\"Positive\"]=total_positive\n",
    "    positive_total_dfs_list.append(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32\n",
    "#This box creates a new column that automatically checks if the triplicate is a positive hit based on specifications defined within.\n",
    "#These specifications can be tinkered with, but it is sort of finicky.\n",
    "positive_single_dfs_list=[]\n",
    "for df in statistics_single_dfs_list:   \n",
    "\n",
    "    single_positive=[]\n",
    "    plate_list=df[\"Plate_ID\"].unique()\n",
    "    for plate in plate_list:\n",
    "        plate_df = df.loc[(df['Plate_ID']==plate)]\n",
    "    \n",
    "        read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "        for read_date in read_date_list:\n",
    "            read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "            trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "            for trip in trip_list:\n",
    "                trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "                for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "                    \n",
    "                    #This will check the sfc and if its over the cutoff you defined, it will move to the next step. Otherwise it will be labelled negative.\n",
    "                    if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                        for si in trips_df[\"SI\"]:\n",
    "                           \n",
    "                            #If the triplicate's si is greater than 2, it will move to the next step. Otherwise it will be labelled as negative.\n",
    "                            if si>=2:\n",
    "                                \n",
    "                                #These next few steps will create an average for the poissons.\n",
    "                                rep_list=[]\n",
    "                                for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                    r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r1)\n",
    "                                for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                    r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r2)\n",
    "                                for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                    r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                    rep_list.append(r3)\n",
    "                                Poisson_avg=statistics.mean(rep_list)\n",
    "                                \n",
    "                                #Now the pvalue cutoffs will be checked.\n",
    "                                for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "                                    \n",
    "                                    #If the pvalue of the ttest is less than .05 OR the poisson avg found above is less than 5, the sfc # will be put into the new positive column.\n",
    "                                    if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                        t=sfc\n",
    "                                        single_positive.append(t)\n",
    "                                    \n",
    "                                    #Otherwise, it will be labelled negative in the positive column.\n",
    "                                    else:\n",
    "                                        f=\"Negative\"\n",
    "                                        single_positive.append(f)\n",
    "                            else:\n",
    "                                f=\"Negative\"\n",
    "                                single_positive.append(f)\n",
    "                    else:\n",
    "                        f=\"Negative\"\n",
    "                        single_positive.append(f)\n",
    "\n",
    "    df[\"Positive\"]=single_positive\n",
    "    positive_single_dfs_list.append(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#33\n",
    "#The final master df is made so that each LED Total DF is put next to eachother.\n",
    "totals_headers=[\"Plate_ID\",\"Read_Date\",\"Triplicate_#\"]\n",
    "\n",
    "#Magic Line!\n",
    "master_totals_combined_df=reduce(lambda x,y : pd.merge(x, y, on = totals_headers), positive_total_dfs_list)\n",
    "\n",
    "#This saves the master Df to the new master excel file that will appear later.\n",
    "master_totals_combined_df.to_excel(writer, sheet_name=\"Master_Totals_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34\n",
    "#The final master df is made so that each LED Total DF is put next to eachother.\n",
    "single_headers=[\"Plate_ID\",\"Read_Date\",\"Triplicate_#\"]\n",
    "\n",
    "#Magic Line!\n",
    "master_singles_combined_df=reduce(lambda x,y : pd.merge(x, y, on = single_headers), positive_single_dfs_list)\n",
    "\n",
    "#This saves the master Df to the new master excel file that will appear later.\n",
    "master_singles_combined_df.to_excel(writer, sheet_name=\"Master_Singles_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#35\n",
    "New_combined_col=[\"Plate_ID\", \"Read_Date\", \"Triplicate_#\"]\n",
    "\n",
    "rep_str=\"rep\"\n",
    "rep_list=[rep_str]*Number_of_negative_wells\n",
    "for i in range(0, (len(rep_list))):\n",
    "    New_combined_col.append(\"rep\"+str(i+1))\n",
    "\n",
    "\n",
    "new_singles_and_combined_df_list=[]\n",
    "    \n",
    "plate_list1=All_Activation_Summary[\"Plate\"].unique()\n",
    "for plate in plate_list1:\n",
    "    plate_df = All_Activation_Summary.loc[(All_Activation_Summary['Plate']==plate)]\n",
    "\n",
    "    Read_date_list=plate_df[\"Read Date\"].unique()\n",
    "    for read_date in Read_date_list:\n",
    "        read_date_df= plate_df.loc[(plate_df[\"Read Date\"]==read_date)]\n",
    "\n",
    "        trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "        for trip in trip_list:\n",
    "            trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "            #taking the unique triplet and joining them together into a list of unique rows\n",
    "            new_rows = [ plate, read_date, trip ]\n",
    "\n",
    "            #what is left after the loops above is a unique set of 3 or 6 SFUs per donor, plate, and pool\n",
    "            #this will now be ran through iterrows, which will isolate each SFU by row and index\n",
    "            for index, row in trips_df.iterrows():\n",
    "\n",
    "                #with these isolated SFUs, a new object is formed\n",
    "                SFU=row[\"Sum_of_all_activation\"]\n",
    "\n",
    "                #this object is now added to the new row set made above\n",
    "                new_rows.append( SFU )\n",
    "\n",
    "                #the new row can now be added into a list to become a new df\n",
    "            new_singles_and_combined_df_list.append(new_rows)\n",
    "\n",
    "#this is taking the new rows and making them into a new df\n",
    "new_singles_and_combined_df=pd.DataFrame( new_singles_and_combined_df_list , columns = New_combined_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#36\n",
    "new_singles_and_combined_df_Average=new_singles_and_combined_df.mean(axis=1, skipna=True)\n",
    "new_singles_and_combined_df_StDev=new_singles_and_combined_df.std(axis=1, skipna=True)\n",
    "    \n",
    "new_singles_and_combined_df[\"Average\"]=new_singles_and_combined_df_Average\n",
    "new_singles_and_combined_df[\"StDev\"]=new_singles_and_combined_df_StDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37\n",
    "NetSFC=[]\n",
    "SI_list=[]\n",
    "poisson_Rep1_list=[]\n",
    "poisson_Rep2_list=[]\n",
    "poisson_Rep3_list=[]\n",
    "\n",
    "plate_list2=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "for plate in plate_list2:\n",
    "    plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "    \n",
    "    \n",
    "    read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "    for read_date in read_date_list:\n",
    "        read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "        Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "        Neg_Avg=float(Neg_df[\"Average\"])\n",
    "\n",
    "        if Neg_Avg>=2:\n",
    "            mu=Neg_Avg\n",
    "        else:\n",
    "            mu=2\n",
    "\n",
    "        trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "        for trip in trip_list:\n",
    "            trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "            \n",
    "\n",
    "            Average_list=trips_df[\"Average\"]\n",
    "            for average in Average_list:\n",
    "\n",
    "                #taking each average minus the negative average of the pool then multiplying it by 5\n",
    "                netsfc=get_netsfu(average)\n",
    "                NetSFC.append(netsfc)\n",
    "\n",
    "                if Neg_Avg > 0:\n",
    "                    SI=(average/Neg_Avg)\n",
    "                    SI_list.append(SI)\n",
    "                else:\n",
    "                    zero_neg_SI=average\n",
    "                    SI_list.append(zero_neg_SI)\n",
    "\n",
    "            Rep1=trips_df[\"rep1\"]\n",
    "            for rep1 in Rep1:\n",
    "                if rep1>=1:\n",
    "                    x1=rep1-1\n",
    "                else:\n",
    "                    x1=0\n",
    "\n",
    "                poisson1=\"{:.1%}\".format(1-stats.poisson.cdf(x1,mu,loc=0))\n",
    "                poisson_Rep1_list.append(poisson1)\n",
    "\n",
    "            Rep2=trips_df[\"rep2\"]\n",
    "            for rep2 in Rep2:\n",
    "                if rep2>=1:\n",
    "                    x2=rep2-1\n",
    "                else:\n",
    "                    x2=0\n",
    "\n",
    "                poisson2=\"{:.1%}\".format(1-stats.poisson.cdf(x2,mu,loc=0))\n",
    "                poisson_Rep2_list.append(poisson2)\n",
    "\n",
    "            Rep3=trips_df[\"rep3\"]\n",
    "            for rep3 in Rep3:\n",
    "                if rep3>=1:\n",
    "                    x3=rep3-1\n",
    "                else:\n",
    "                    x3=0\n",
    "\n",
    "                poisson3=\"{:.1%}\".format(1-stats.poisson.cdf(x3,mu,loc=0))\n",
    "                poisson_Rep3_list.append(poisson3)\n",
    "\n",
    "new_singles_and_combined_df[\"NetSFC_per_million\"]=NetSFC\n",
    "new_singles_and_combined_df[\"SI\"]=SI_list\n",
    "new_singles_and_combined_df[\"Poisson_Rep1\"]=poisson_Rep1_list\n",
    "new_singles_and_combined_df[\"Poisson_Rep2\"]=poisson_Rep2_list\n",
    "new_singles_and_combined_df[\"Poisson_Rep3\"]=poisson_Rep3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#38\n",
    "ttests=[]\n",
    "\n",
    "plate_list3=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "for plate in plate_list3:\n",
    "    plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "    \n",
    "    \n",
    "    read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "    for read_date in read_date_list:\n",
    "        read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "            \n",
    "        Neg_triplicates=[]\n",
    "        Neg_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==\"neg\")]\n",
    "        Neg_triplicates=get_replicates_from_row(Neg_df)\n",
    "\n",
    "        trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "        for trip in trip_list:\n",
    "            trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "            triplicates=get_replicates_from_row(trips_df)\n",
    "\n",
    "\n",
    "            ttest_result=get_pvalue(stats.ttest_ind(triplicates, Neg_triplicates, equal_var = False, nan_policy='omit'))\n",
    "            ttests.append(ttest_result)\n",
    "\n",
    "\n",
    "new_singles_and_combined_df[\"Ttest_pvalue\"]=ttests        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#39\n",
    "new_singles_and_combined_positive=[]\n",
    "plate_list4=new_singles_and_combined_df[\"Plate_ID\"].unique()\n",
    "for plate in plate_list4:\n",
    "    plate_df = new_singles_and_combined_df.loc[(new_singles_and_combined_df['Plate_ID']==plate)]\n",
    "    \n",
    "    \n",
    "    read_date_list=plate_df[\"Read_Date\"].unique()\n",
    "    for read_date in read_date_list:\n",
    "        read_date_df=plate_df.loc[(plate_df[\"Read_Date\"]==read_date)]\n",
    "\n",
    "        #repeating now for the unique pool responses in the plate df\n",
    "        trip_list=read_date_df[\"Triplicate_#\"].unique()\n",
    "        for trip in trip_list:\n",
    "            trips_df=read_date_df.loc[(read_date_df[\"Triplicate_#\"]==trip)]\n",
    "\n",
    "            for sfc in trips_df[\"NetSFC_per_million\"]:\n",
    "                if sfc >=Number_of_sfcs_necessary_to_be_considered:\n",
    "                    for si in trips_df[\"SI\"]:\n",
    "                        if si>=2:\n",
    "                            rep_list=[]\n",
    "                            for REP1 in trips_df[\"Poisson_Rep1\"]:\n",
    "                                r1=float(REP1.replace(\"%\",\"\"))\n",
    "                                rep_list.append(r1)\n",
    "                            for REP2 in trips_df[\"Poisson_Rep2\"]:\n",
    "                                r2=float(REP2.replace(\"%\",\"\"))\n",
    "                                rep_list.append(r2)\n",
    "                            for REP3 in trips_df[\"Poisson_Rep3\"]:\n",
    "                                r3=float(REP3.replace(\"%\",\"\"))\n",
    "                                rep_list.append(r3)\n",
    "                            Poisson_avg=statistics.mean(rep_list)\n",
    "                            for pvalue in trips_df[\"Ttest_pvalue\"]:\n",
    "                                if pvalue <=.05 or Poisson_avg <= 5:\n",
    "                                    t=sfc\n",
    "                                    new_singles_and_combined_positive.append(t)\n",
    "                                else:\n",
    "                                    f=\"Negative\"\n",
    "                                    new_singles_and_combined_positive.append(f)\n",
    "                        else:\n",
    "                            f=\"Negative\"\n",
    "                            new_singles_and_combined_positive.append(f)\n",
    "                else:\n",
    "                    f=\"Negative\"\n",
    "                    new_singles_and_combined_positive.append(f)\n",
    "\n",
    "new_singles_and_combined_df[\"Positive\"]=new_singles_and_combined_positive\n",
    "new_singles_and_combined_df.to_excel(writer, sheet_name=\"Master_Singles+Combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40\n",
    "#This saves the Master Excel file to path defined in the writer function.\n",
    "#If unchanged, it will end up on your desktop.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
